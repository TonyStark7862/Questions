Risk category and sub-category,Ref.,Question,Risk Scores,Justification

1. Description of the AI Technology,,,,"Please provide comprehensive explanations with supporting evidence. As the AI solution owner, you are responsible for ensuring completeness and accuracy of all information provided."

AI Technology Purpose,1.1.1,"Has the AI solution owner clearly articulated the purpose of the AI Technology/Model? Please describe in detail the primary business objective, strategic alignment with organizational goals, specific business problems it aims to solve, and quantifiable expected benefits. How does this solution address specific business needs that could not be addressed through conventional means?",1,

Development Ownership,1.1.2,"Has the AI solution owner specified the desk/team that developed the AI technology? Please provide details on organizational structure, reporting lines, team composition (roles and responsibilities), relevant expertise and credentials in AI/ML implementations, and previous experience with similar technologies. Have any external vendors or partners been involved in development and if so, how is their work governed?",1,

Target User Assessment,1.1.3,"Has the AI solution owner comprehensively identified the target users and deployment regions for the AI Technology? Please detail regional considerations including: regulatory environments, language requirements, cultural factors, data privacy implications, and anticipated adoption challenges for each deployment region. How have region-specific risks been identified and mitigated?",1,

User Onboarding,1.1.4,"Has the AI solution owner developed a comprehensive user onboarding strategy? Detail the training methodologies, documentation standards, support mechanisms, and competency verification processes for end users. How will user proficiency with the AI Technology be assessed and maintained?",1,

Operational Integration,1.1.5,"Has the AI solution owner documented how the AI Technology will be operationalized within existing business processes? Include integration points, dependencies on other systems, change management requirements, training needs for users, and performance expectations under normal and peak operational conditions. What operational risks have been identified and how are they mitigated?",1,

Business Context Analysis,1.1.6,"Has the AI solution owner analyzed the broader business context in which this AI Technology will operate? This should include market conditions, competitive landscape, regulatory environment, and organizational readiness. How does this solution address specific business challenges or opportunities within this context? What business metrics will measure success?",1,

Application Scope Definition,1.1.7,"Has the AI solution owner provided a well-articulated application scope detailing exactly what the AI Technology will and will not do? This should include functional boundaries, use case specifications, explicit exclusions, integration requirements, and scaling considerations. How were these boundaries determined, and how are they communicated to users and stakeholders?",1,

Technical Architecture Documentation,1.1.8,"Has the AI solution owner detailed the technical implementation architecture of the AI model? This should include framework selection (e.g., transformer-based encoding), infrastructure requirements, deployment methodology, component interaction, data flow, API specifications, security measures, and scalability considerations. How does this architecture support the business requirements while ensuring appropriate controls?",1,

Development Methodology Documentation,1.1.9,"Has the AI solution owner documented the development methodology utilized for this AI Technology? This should include development lifecycle approach, validation strategy, quality assurance framework, testing protocols, code review procedures, and release management process. How were development standards established and enforced throughout the development lifecycle?",1,

Business Requirement Alignment,1.1.10,"Has the AI solution owner verified alignment between business requirements and AI Technology capabilities? Detail the requirement-gathering methodology, traceability matrix, validation process, and stakeholder sign-off procedures. How are requirement gaps or misalignments identified and addressed?",1,

Assumptions and Limitations,,,, 

Key Assumptions Documentation,1.2.1,"Has the AI solution owner documented all key assumptions made during model development with their corresponding implications and justifications? This should include assumptions related to data representativeness, problem formulation, algorithm selection, hyperparameter choice, business environment stability, and regulatory interpretation. How were these assumptions validated, and what processes exist to reassess them over time?",2,

Data Source Limitation Analysis,1.2.2,"Has the AI solution owner analyzed any single-source or limited-source data dependencies? For instance, if the AI Model uses a single source of news/data (e.g., Bloomberg terminal), has the solution owner assessed implications for potentially missing risk events or information? Please include quantification of potential blind spots and their business impact. What alternative data sources were considered and why were they not selected?",2,

Statistical Assumption Validation,1.2.3,"Has the AI solution owner identified and validated all statistical assumptions underlying the model? This should include distribution assumptions, independence assumptions, stationarity assumptions, and any other mathematical premises upon which the model is built. What testing methodologies were employed to validate these assumptions, and what contingencies exist if these assumptions prove invalid?",2,

Domain Assumption Sourcing,1.2.4,"Has the AI solution owner documented all domain-specific assumptions made regarding business rules, market behavior, user interaction patterns, or expert knowledge? How were these domain assumptions sourced, validated, and incorporated into the model? What subject matter experts were consulted and how was their input incorporated into the model design?",2,

Regional Limitation Assessment,1.2.5,"Has the AI solution owner assessed known geographical or regional limitations of the model? This should include language constraints, market-specific data limitations, regulatory compliance gaps, and cultural context limitations. How has the solution owner quantified the potential impact of these limitations on model performance and business outcomes? What mitigation strategies address these regional limitations?",2,

Temporal Limitation Analysis,1.2.6,"Has the AI solution owner analyzed any temporal limitations of the model? This should include historical data scope limitations, seasonality effects, trending dependencies, time-sensitivity of data, and projection limitations. How are these temporal factors addressed in the model design and output interpretation? What processes exist to update the model as temporal conditions change?",2,

Output Bias Comprehensive Assessment,1.2.7,"Has the AI solution owner conducted a comprehensive bias analysis of potential model outputs? For example, has the solution owner assessed if outputs could be biased towards larger organizations, US-based entities, or English-language content? Have all identified biases been categorized by severity (High/Medium/Low), potential business impact, and reputational risk? What monitoring and remediation strategies address these biases?",3,

Sensitivity Analysis Documentation,1.2.8,"Has the AI solution owner conducted and documented a detailed sensitivity analysis of model performance relative to key assumptions? Which assumptions, if violated, would most significantly impact model performance? Has the solution owner quantified the expected performance degradation and business impact under various assumption violation scenarios? What triggers would indicate assumption violations in production?",2,

Mitigation Controls Framework,1.2.9,"Has the AI solution owner implemented and documented controls, guardrails, and mitigation strategies addressing identified limitations? This should include monitoring mechanisms, override protocols, validation thresholds, alerting systems, review processes, and contingency plans. How has the solution owner tested and validated these controls under various scenarios? What is the escalation process when controls indicate issues?",2,

Limitation Communication Strategy,1.2.10,"Has the AI solution owner established processes for documenting, communicating, and incorporating model limitations into business processes? How are limitations reflected in model governance, user training, and operational procedures? What disclosure mechanisms ensure transparency regarding model limitations to all relevant stakeholders? How frequently are limitations reassessed?",2,

Assumption Update Procedure,1.2.11,"Has the AI solution owner established procedures for updating assumptions as business conditions, data characteristics, or regulatory requirements change? What triggers a reassessment of model assumptions? How are assumption changes documented, validated, and governed? What testing is required before updated assumptions are implemented in production?",2,

Model Adaptability Assessment,1.2.12,"Has the AI solution owner assessed the adaptability of the model to changing conditions? What mechanisms allow the model to remain effective as business requirements evolve? How quickly can the model incorporate new assumptions or limitations? What testing framework validates model adaptability?",2,

Inputs Assessment,,,, 

Input Parameters Comprehensive Inventory,1.3.1,"Has the AI solution owner provided a comprehensive inventory of all AI Technology inputs? This should include data sources, structured fields, unstructured data, reference data, market data, configuration parameters, and triggering conditions. For each input, has the solution owner detailed the source, format, frequency, quality controls, and criticality to model performance? What governance controls exist for each input category?",2,

Pre-processing Requirements Documentation,1.3.2,"Has the AI solution owner documented all data pre-processing requirements? This should include cleansing methodologies, normalization techniques, tokenization processes, named entity recognition, feature engineering steps, data transformation pipelines, and quality assurance checks. How are these pre-processing steps validated, monitored, and governed? What controls prevent pre-processing drift?",2,

Opaque Parameters Governance,1.3.3,"Has the AI solution owner identified all opaque parameters or expert judgments incorporated in the model? For each parameter, has the solution owner documented derivation methodology, validation approach, governance controls, review frequency, authorized modifiers, documentation requirements, and impact on model outcomes? What processes control changes to these parameters and ensure appropriate oversight?",3,

Parameter Update Governance Framework,1.3.4,"Has the AI solution owner established a governance framework for parameter updates? This should include authorization protocols, testing requirements, validation methodologies, documentation standards, version control procedures, and audit trails. How frequently are parameters updated, what triggers these updates, and how is update effectiveness measured? What rollback procedures exist if updates cause issues?",2,

Update Frequency Justification Analysis,1.3.5,"Has the AI solution owner provided a detailed justification for the update frequency of model parameters and expert judgments? This should include analysis of business cycle considerations, market volatility factors, data refresh rates, model drift detection, and alignment with business decision timeframes. Is the established frequency sufficient for the intended use case? What analysis supports this determination?",2,

RAG Implementation Details,1.3.6,"If Retrieval-Augmented Generation (RAG) is used for optimizing model output, has the AI solution owner provided comprehensive technical details? This should include retrieval methodology, knowledge source selection, integration approach, relevance ranking, context incorporation, query formulation, and performance metrics. How is RAG effectiveness measured and optimized? What governance controls are specific to the RAG implementation?",2,

Internal Data Source Security,1.3.7,"Has the AI solution owner cataloged all XYZ internal data sources (Sharepoint, Jira, Gitlab, Confluence, etc.) utilized by the model? For each source, has the solution owner detailed access controls, security measures, data lineage, quality assurance, refresh mechanisms, backup procedures, and compliance status? How is data consistency and accuracy ensured across internal sources? What compliance requirements apply to each data source?",2,

Feeder Models Integration Framework,1.3.8,"Has the AI solution owner identified all feeder AI Models or Agents (internal or third-party) whose outputs are used by this AI Model? For each dependency, has the solution owner documented integration methodology, data exchange format, quality controls, dependency management, failure handling, version compatibility, and performance monitoring? What contingency plans exist for feeder model failures or performance degradation?",3,

Data Quality Management System,1.3.9,"Has the AI solution owner established a comprehensive framework for ensuring input data quality? This should include quality metrics, monitoring frequency, anomaly detection, remediation procedures, quality thresholds, validation checks, and escalation protocols. How are data quality issues identified, addressed, and prevented? What governance controls oversee data quality management?",2,

Input Validation Comprehensive Methodology,1.3.10,"Has the AI solution owner documented methodologies for validating inputs? This should include range checks, consistency validations, reference data comparison, anomaly detection, trend analysis, and cross-validation. How are validation failures handled and remediated? What metrics track validation effectiveness, and what thresholds trigger intervention?",2,

Input Drift Monitoring,1.3.11,"Has the AI solution owner implemented systems to detect and address input drift? What metrics and thresholds identify significant shifts in input distributions or characteristics? What procedures respond to detected drift, and what governance controls oversee drift management? How frequently is drift assessed, and what reporting mechanisms communicate drift findings?",3,

Input Lineage Documentation,1.3.12,"Has the AI solution owner established comprehensive input data lineage tracking? What systems document the origin, transformations, and usage of all input data? How is lineage information maintained and made available for audit and governance purposes? What controls ensure lineage completeness and accuracy?",2,

Input Security Controls,1.3.13,"Has the AI solution owner implemented appropriate security controls for all inputs? What measures protect data confidentiality, integrity, and availability throughout the input lifecycle? How are security controls tested and validated? What compliance requirements apply to input security, and how is compliance verified?",3,

Input Change Management,1.3.14,"Has the AI solution owner established a change management process for inputs? What procedures govern the addition, modification, or removal of input sources or parameters? What testing and validation requirements apply to input changes? How are changes documented and communicated to stakeholders?",2,

Manual Input Controls,1.3.15,"Has the AI solution owner identified any manual inputs to the model and established appropriate controls? What validation checks, authorization requirements, and documentation standards apply to manual inputs? How are manual input errors prevented and detected? What segregation of duties exists for manual input processes?",3,

Calibration Assessment,,,, 

Calibration Comprehensive Methodology,1.4.1,"Has the AI solution owner documented a detailed calibration methodology for AI Technology parameters and/or methodology? This should include calibration objectives, parameter selection criteria, optimization techniques, validation approaches, performance metrics, and recalibration triggers. How is the calibration process documented and governed? What testing validates calibration effectiveness?",2,

Calibration Implementation Specifications,1.4.2,"Has the AI solution owner detailed the technical implementation of the calibration process? This should include algorithmic approach, optimization techniques, convergence criteria, processing requirements, automation level, and execution frequency. Has the solution owner provided supporting rationale for the selected implementation approach? What alternatives were considered and why were they rejected?",2,

Calibration Frequency Determination,1.4.3,"Has the AI solution owner specified and justified the frequency of model calibration activities? This should include analysis of sensitivity to market conditions, data refresh considerations, model drift patterns, business cycle alignment, and operational constraints. How was this frequency determined to be appropriate? What factors would trigger out-of-cycle calibration?",2,

External Knowledge Integration Methodology,1.4.4,"Has the AI solution owner documented how external knowledge is integrated into the model? This should include knowledge sources, selection criteria, integration methodology, quality assurance, and maintenance procedures. How is the quality and relevance of external knowledge assessed and maintained? What governance controls oversee external knowledge integration?",2,

Fine-tuning Comprehensive Process,1.4.5,"If applicable, has the AI solution owner documented the model fine-tuning process? This should include dataset selection, data preparation, training methodology, hyperparameter optimization, convergence criteria, validation approach, and performance evaluation. How are overfitting and data contamination prevented? What governance controls oversee the fine-tuning process?",2,

Calibration Validation Framework,1.4.6,"Has the AI solution owner established a validation process for calibration effectiveness? This should include validation dataset selection, performance metrics, acceptance criteria, comparative analysis, and statistical testing. How are validation results documented and incorporated into governance? What thresholds determine calibration success or failure?",2,

Parameter Sensitivity Analysis,1.4.7,"Has the AI solution owner conducted and documented a sensitivity analysis of model performance to calibration parameters? Which parameters have the most significant impact on model performance and business outcomes? How is this sensitivity information used in calibration governance and risk management? What controls address highly sensitive parameters?",3,

Calibration Documentation Standards,1.4.8,"Has the AI solution owner established documentation standards for calibration activities? This should include parameter values, methodological choices, validation results, approval records, and version control. How is calibration history maintained and made available for audit and governance purposes? What retention requirements apply to calibration documentation?",2,

Calibration Governance Framework,1.4.9,"Has the AI solution owner established an oversight framework for calibration activities? This should include roles and responsibilities, approval authorities, review processes, challenge mechanisms, and escalation procedures. How is independence ensured in calibration validation? What governance committees oversee calibration activities?",2,

Knowledge Enhancement Methods Documentation,1.4.10,"Has the AI solution owner documented any additional knowledge enhancement methods implemented beyond standard calibration? This may include domain adaptation, transfer learning, ensemble techniques, or knowledge distillation. How are these enhancements validated and governed? What performance improvements result from these enhancements?",2,

Calibration Automation Assessment,1.4.11,"Has the AI solution owner assessed the feasibility and appropriateness of calibration automation? What aspects of calibration are automated versus manual? What controls ensure automated calibration accuracy and reliability? What human oversight applies to automated calibration processes?",2,

Calibration Testing Framework,1.4.12,"Has the AI solution owner established a comprehensive testing framework for calibration? What test cases, scenarios, and edge conditions validate calibration effectiveness? How are test results documented and incorporated into the calibration process? What acceptance criteria determine calibration success?",2,

Calibration Versioning Controls,1.4.13,"Has the AI solution owner implemented version control for calibration configurations and parameters? How are calibration versions tracked, documented, and governed? What procedures control migration between calibration versions? How is backward compatibility assessed when calibration changes occur?",2,

Numerical Methods,,,, 

Numerical Methods Comprehensive Details,1.5.1,"Has the AI solution owner provided comprehensive details of all numerical methods employed? This should include analytical approaches, semi-analytical techniques, approximation methods, and computational algorithms. For each method, has the solution owner explained the mathematical foundation, implementation details, and appropriateness for the intended application? What factors influenced method selection?",2,

Method Selection Justification,1.5.2,"Has the AI solution owner justified the selection of the numerical methods used in the model? This should include comparative analysis of alternative approaches, performance considerations, accuracy requirements, computational efficiency, and domain suitability. Why are the selected methods optimal for this application? What limitations were identified in the selected methods?",2,

Non-analytical Methods Documentation,1.5.3,"If non-analytical methods are used (e.g., Monte Carlo Simulations, Finite Difference Method), has the AI solution owner provided detailed technical specifications? This should include implementation approach, sampling techniques, convergence criteria, error bounds, and computational requirements. How are these methods validated? What governance controls apply specifically to non-analytical methods?",3,

Numerical Inputs Specification,1.5.4,"Has the AI solution owner specified all numerical inputs and their recommended levels/values? For each input, has the solution owner documented derivation methodology, acceptable ranges, sensitivity impact, and validation approach? How were these values determined to be appropriate for the model's intended use? What processes control changes to numerical inputs?",2,

Accuracy Validation Framework,1.5.5,"Has the AI solution owner detailed the methodology for validating numerical accuracy? This should include benchmark comparisons, error analysis, convergence testing, edge case validation, and performance under stress conditions. What are the established accuracy thresholds and how are they enforced? What remediation processes address accuracy issues?",2,

Error Management System,1.5.6,"Has the AI solution owner described the framework for managing numerical errors? This should include error detection methods, propagation analysis, remediation procedures, and impact assessment. How are numerical errors logged, analyzed, and addressed? What thresholds trigger intervention for numerical errors?",2,

Computational Efficiency Analysis,1.5.7,"Has the AI solution owner analyzed the computational efficiency of the numerical methods employed? This should include performance benchmarks, resource utilization, scalability analysis, and optimization techniques. How is computational performance monitored and maintained? What thresholds indicate unacceptable computational performance?",2,

Mathematical Foundation Documentation,1.5.8,"Has the AI solution owner provided a detailed explanation of the mathematical foundation of the model? This should include underlying theories, principles, equations, and algorithms. How is mathematical correctness ensured and validated? What peer review or expert validation has been applied to the mathematical foundation?",3,

Numerical Stability Assessment,1.5.9,"Has the AI solution owner assessed the numerical stability of the model under various conditions? This should include input variations, parameter changes, computational environment differences, and edge cases. How is stability tested and maintained? What controls prevent numerical instability in production?",2,

Method Documentation Standards,1.5.10,"Has the AI solution owner established documentation standards for numerical methods? This should include mathematical specifications, implementation details, validation results, and limitations. How is this technical information made accessible for review and governance? What documentation quality controls ensure completeness and accuracy?",2,

Precision Requirements Documentation,1.5.11,"Has the AI solution owner specified precision requirements for numerical calculations? What precision levels are necessary for accurate results, and how were these levels determined? How is required precision maintained across different processing environments? What validation confirms precision requirements are met?",2,

Numerical Method Versioning,1.5.12,"Has the AI solution owner implemented version control for numerical methods? How are changes to numerical methods tracked, tested, and governed? What validation ensures consistent performance across numerical method versions? What documentation standards apply to numerical method versioning?",2,

Numerical Method Benchmarking,1.5.13,"Has the AI solution owner conducted benchmarking of numerical methods against industry standards or alternative implementations? What comparative analysis validates the performance and accuracy of implemented methods? How are benchmarking results incorporated into method selection and optimization?",2,

Outputs Assessment,,,, 

Output Comprehensive Specification,1.6.1,"Has the AI solution owner provided a comprehensive specification of all AI Technology outputs? This should include data types, formats, structures, volumes, frequencies, and delivery mechanisms. How do these outputs align with business requirements and user needs? What governance controls apply to output specifications?",2,

Output Business Context Documentation,1.6.2,"Has the AI solution owner documented how model outputs are used within business processes? This should include decision support applications, automated processes, user interfaces, downstream systems, and external communications. How critical are these outputs to business operations and decision-making? What contingency plans exist for output failures?",3,

UI Content Generation Specification,1.6.3,"If applicable, has the AI solution owner detailed how the model generates content for UI users? This should include content types, formatting requirements, personalization capabilities, delivery mechanisms, and quality control measures. How is UI content validated for accuracy and appropriateness? What governance controls apply specifically to UI-destined content?",2,

SQL Query Generation Security Controls,1.6.4,"If the model generates SQL queries from natural language prompts, has the AI solution owner documented security measures and validation procedures? This should include query construction methodology, security controls, performance optimization, validation procedures, and error handling. How are injection vulnerabilities and unauthorized access prevented? What governance oversees SQL generation?",3,

Output Format Comprehensive Specifications,1.6.5,"Has the AI solution owner detailed all output format specifications? This should include data structures, file formats, API responses, visualization requirements, and integration specifications. Are these formats customizable based on user needs or system requirements? What validation ensures output format conformance?",2,

Output Quality Control Framework,1.6.6,"Has the AI solution owner established a framework for ensuring output quality? This should include validation procedures, accuracy checks, consistency verification, anomaly detection, and review processes. How are quality issues identified, addressed, and prevented? What metrics track output quality, and what thresholds trigger intervention?",3,

Output Interpretability Assessment,1.6.7,"Has the AI solution owner assessed the interpretability of model outputs for intended users? This should include clarity of presentation, contextual information, confidence metrics, underlying factors, and explanatory notes. How is user comprehension of outputs ensured? What training and documentation support output interpretation?",2,

Business Process Integration Documentation,1.6.8,"Has the AI solution owner detailed how model outputs are integrated into broader business processes? This should include system interfaces, workflow integration, decision support frameworks, and operational procedures. How seamless is this integration? What controls validate integration effectiveness and prevent integration failures?",2,

Output Monitoring System,1.6.9,"Has the AI solution owner established a monitoring framework for model outputs? This should include performance metrics, drift detection, anomaly identification, quality thresholds, and alerting mechanisms. How are output trends and patterns analyzed? What governance oversees output monitoring, and what escalation procedures address identified issues?",3,

Feedback Collection Mechanisms,1.6.10,"Has the AI solution owner implemented mechanisms for collecting and incorporating user feedback on model outputs? This should include feedback channels, analysis methodology, improvement processes, and implementation timelines. How is output quality continuously improved based on feedback? What governance oversees the feedback incorporation process?",2,

Output Validation Methodology,1.6.11,"Has the AI solution owner established methodologies for validating outputs? This should include accuracy checks, consistency verification, reference comparisons, reasonableness assessments, and edge case testing. How comprehensive is output validation, and what controls ensure validation effectiveness?",3,

Output Versioning Control,1.6.12,"Has the AI solution owner implemented version control for output formats and specifications? How are changes to outputs tracked, communicated, and governed? What backward compatibility considerations apply to output changes? What testing validates output version transitions?",2,

Output Security Controls,1.6.13,"Has the AI solution owner implemented security controls for model outputs? This should include confidentiality measures, integrity protections, access controls, and transmission security. How are outputs protected throughout their lifecycle? What compliance requirements apply to output security?",3,

Output Lineage Tracking,1.6.14,"Has the AI solution owner established lineage tracking for outputs? How are output origins, transformations, and destinations documented? What audit capabilities allow tracing outputs back to inputs and processing steps? How is lineage information maintained and made available for governance purposes?",2,

Output Performance Metrics,1.6.15,"Has the AI solution owner defined and documented performance metrics for outputs? What measures assess output accuracy, timeliness, completeness, and business value? How are these metrics tracked and reported? What thresholds indicate acceptable versus unacceptable performance? What governance oversees output performance management?",3,

2. Model Assessment,,,,"Based on the information provided in Section 1, this section determines whether the AI Technology qualifies as a Model under the governance framework. A Model is defined as a quantitative method, system or approach, that applies statistical, economic, financial, or mathematical theories, techniques, and assumptions to process input data into estimates."

Model Definition Comprehensive Assessment,2.1.1,"Has the AI solution owner evaluated whether this AI Technology meets the definition of a Model? A Model is defined as: ""A quantitative method, system or approach, that applies statistical, economic, financial, or mathematical theories, techniques, and assumptions to process input data into quantitative or qualitative estimates."" Please provide a detailed assessment against each component of this definition, with specific evidence from the AI Technology implementation.",3,

Inherent Uncertainty Analysis,2.1.2,"Has the AI solution owner assessed whether the outputs of the AI Technology are inherently uncertain (estimates, forecasts, predictions, or projections)? Please provide a detailed analysis of the sources and nature of this uncertainty and their implications for model classification. What metrics quantify the level of uncertainty in model outputs?",3,

Approach-specific Assumptions Identification,2.1.3,"Has the AI solution owner identified and analyzed all approach-specific assumptions that would qualify this as a Model? As specified in Section 1.2, provide a detailed assessment of how these assumptions impact model behavior, limitations, and reliability. Which assumptions are most critical to model classification, and why?",3,

Input Uncertainty Comprehensive Analysis,2.1.4,"Has the AI solution owner evaluated whether the approach relies on inputs that are uncertain? This includes inputs that are wholly/partially qualitative, opaque, or based on expert judgment. Please detail how these uncertain inputs affect model classification, providing specific examples from the AI Technology implementation. What controls address input uncertainty risks?",3,

Multiple Model Dependency Assessment,2.1.5,"Has the AI solution owner assessed whether the approach relies on outputs of other Models, especially multiple Models? Please provide a detailed dependency map showing integration points, data flows, and dependency criticality. How do these dependencies impact model classification and risk profile?",3,

Calibration Requirement Analysis,2.1.6,"Has the AI solution owner determined whether any of the approach's inputs and/or methodology require calibration? With reference to Section 1.4, provide a detailed analysis of calibration requirements and their significance for model classification. How do calibration needs impact the AI Technology's risk profile?",2,

Non-analytical Methodology Evaluation,2.1.7,"Has the AI solution owner evaluated whether the approach is based on a non-analytical method (e.g., Monte Carlo Simulations, Finite Difference Method)? Please detail the methodology and its implications for model classification. What specific aspects of the non-analytical approach impact model risk and classification?",3,

Exclusion Justification Documentation,2.1.8,"If the above conditions are not applicable to the quantitative approach but the AI solution owner believes there are special reasons to consider it a Model, has detailed justification been provided? Please detail the unique characteristics that warrant Model classification despite not meeting standard criteria. What governance or business considerations support this classification?",3,

Model Classification Justification,2.1.9,"If at least one of the above conditions is applicable but the AI solution owner does not identify the quantitative approach as a Model, has detailed justification been provided? Please explain why Model classification is not appropriate despite meeting one or more criteria. What specific factors outweigh the standard classification criteria?",3,

Assessment Conclusion Documentation,2.1.10,"Has the AI solution owner provided a definitive conclusion (Model or non-Model) based on the comprehensive assessment above? This determination establishes the governance requirements applicable to this AI Technology. Please provide the owner's conclusion with supporting rationale. If classified as a Model, what tier of governance applies?",3,

Model Classification Review Process,2.1.11,"Has the AI solution owner established a process for reviewing the Model classification determination over time? What triggers would prompt reassessment of the classification? How is the classification decision documented and communicated to relevant stakeholders? What governance oversees classification decisions?",2,

3. Model Complexity Assessment,,,,"Model Complexity reflects the inherent risks within each component of the AI Technology, including inputs/data, assumptions, methodology, implementation and outputs. Complexity is assessed using risk factors and assigned based on the sum of risk factor scores (out of 19 total possible points)."

Industry Acceptance Comprehensive Assessment,3.1.1,"Has the AI solution owner evaluated whether this is a non-standard Model based on a special set of Model assumptions and/or a large set of non-standard inputs? Per guidance, a non-standard Model is inherently more risky and should be rated higher. Please provide detailed analysis of all non-standard elements and their risk implications. How do these non-standard elements impact model governance and controls?",3,

Assumption Suitability Analysis,3.1.2,"Has the AI solution owner assessed whether Model assumptions are fully suitable to Model intended use? A Model is inherently more risky if Model assumptions are NOT fully suitable to intended use. Please provide a comprehensive assessment of assumption adequacy relative to business requirements, noting any gaps or misalignments. What controls mitigate risks from assumption limitations?",3,

Input Sensitivity Comprehensive Analysis,3.1.3,"Has the AI solution owner conducted a detailed sensitivity analysis of the Model to its assumptions and Expert Judgment inputs? A Model is riskier if its outputs are highly sensitive to one or more of its major assumptions or expert judgment inputs. Please quantify sensitivity measures for key inputs and explain their implications for model risk. What thresholds determine high, medium, or low sensitivity?",3,

Hyperparameter Complexity Assessment,3.1.4,"Has the AI solution owner evaluated whether the algorithm contains a high number of hyperparameters (e.g., learning rate, number of features, weighting)? A Model is inherently more risky if the algorithm contains a high number of hyperparameters. Please catalog all hyperparameters and their optimization methodology. How are hyperparameter changes controlled and validated?",3,

Data Complexity Analysis,3.1.5,"Has the AI solution owner assessed Model Risk associated with complex data structures, unstructured data, or low-quality inputs? Model Risk increases with complex data structures, unstructured data, or low-quality inputs. Please provide a detailed analysis of data complexity factors and their impact on model risk. What controls mitigate risks from data complexity?",3,

Feature Volume Risk Assessment,3.1.6,"Has the AI solution owner evaluated whether the AI Model uses a large number of features/variables? Model Risk increases when a large number of features/variables increases Model Risk. Please quantify feature complexity and its impact on model risk. What controls address risks from high feature volume, and how is feature selection justified?",3,

Performance Limitation Analysis,3.1.7,"Has the AI solution owner assessed Model Risk arising from Model performance limitations under current market or extreme conditions? Please provide detailed analysis of performance under various scenarios and stress conditions. What performance thresholds indicate unacceptable model behavior, and what contingency plans address performance failures?",3,

Infrastructure Automation Assessment,3.1.8,"Has the AI solution owner evaluated the level of Model infrastructure automation, including straight-through processing capabilities? An automated Model infrastructure reduces the risk of operational error. Please detail automation level, control points, and risk reduction measures. What manual intervention points remain, and how are they controlled?",2,

Model Dependency Risk Analysis,3.1.9,"Has the AI solution owner assessed Model Risk associated with complex or large number of feeder Models? Model Risk increases when complex or large number of feeder Models are used. Please provide a detailed dependency map and risk analysis. What controls mitigate risks from model dependencies, and what contingency plans address dependency failures?",3,

Complexity Score Calculation Documentation,3.1.10,"Has the AI solution owner calculated the total complexity score based on the assessments above? With reference to the scoring matrix (Low <=5, Medium 6-11, High >=12), please provide detailed scoring calculation and justification for each factor. The maximum score is 19 points. How does the final score reflect the overall complexity risk profile?",3,

Complexity Classification Determination,3.1.11,"Based on the calculated score, has the AI solution owner assigned the appropriate Complexity Classification (Low/Medium/High)? This classification has direct implications for the Model Tier assignment and governance requirements. Please document the assigned classification with supporting evidence. How does this classification align with qualitative assessment of model complexity?",3,

Complexity Governance Controls,3.1.12,"Has the AI solution owner established governance controls commensurate with the assessed complexity level? Please detail the specific governance mechanisms, documentation requirements, review processes, and testing protocols aligned with the complexity classification. How do these controls mitigate the identified complexity risks?",2,

Complexity Reassessment Framework,3.1.13,"Has the AI solution owner established a framework for reassessing complexity over time? What triggers would prompt a complexity reassessment? How frequently is complexity formally reviewed? What governance oversees complexity determinations, and how are changes to complexity classification managed?",2,

Risk category and sub-category,Ref.,Question,Risk Scores,Justification

4. Model Materiality Assessment,,,,"Model Materiality is determined by Model Use and the level of human review/oversight into the AI technology development, training and output (Human in the Loop - HITL)"

External-Facing Usage Evaluation,4.1.1,"Provide a comprehensive assessment of whether model outputs are used for external-facing activities (e.g., client, regulator). In accordance with the Model Use Rating table, external-facing use represents High materiality. Detail all external usage scenarios, their frequency, business significance, and associated regulatory considerations.",3,

Internal Impact Evaluation,4.1.2,"Evaluate whether model outputs are used for internal facing activities with direct impact on external output (e.g., risk management). Per the Model Use Rating table, this represents Medium materiality. Document the impact flow from internal use to external outcomes, providing specific examples of how internal usage influences external-facing decisions or outputs.",2,

Operational Efficiency Usage Assessment,4.1.3,"Assess whether model outputs are used for internal facing activities aimed at increasing operational efficiencies (e.g., data analytics, chatbot, translation). Per the Model Use Rating table, this represents Low materiality. Quantify the efficiency benefits, usage patterns, and business value derived from these operational applications.",1,

Human Oversight Implementation Assessment,4.1.4,"Document the level of Human in the Loop implementation based on the HITL assessment matrix. This may be classified as Minimal (basic prompt engineering, no systematic review, no feedback loop), Standard (structured management, regular sampling, basic feedback), or Enhanced (comprehensive monitoring, active optimization, documented process). Provide detailed evidence supporting the assigned level of human oversight.",3,

Prompt Engineering Methodology Evaluation,4.1.5,"Detail the sophistication of prompt engineering implemented for this AI solution. This may range from basic prompt engineering only (Minimal), to structured prompt management with version control (Standard), to active prompt optimization based on user feedback (Enhanced). Document the prompt management methodology, governance controls, and version management approach.",2,

Output Review Process Documentation,4.1.6,"Describe the systematic review process established for model outputs. This may range from no systematic review (Minimal), to regular sampling and review (Standard), to comprehensive output monitoring with quality control (Enhanced). Document review procedures, sampling methodology, review frequency, and remediation workflows for identified issues.",2,

Feedback Collection Framework Assessment,4.1.7,"Evaluate the feedback collection process implemented for this AI solution. This may range from no feedback loop (Minimal), to basic feedback collection from end users (Standard), to systematic collection and incorporation of SME feedback (Enhanced). Document the feedback mechanisms, analysis methodology, implementation procedures, and governance oversight of the feedback process.",2,

Retrieval Quality Evaluation Process,4.1.8,"Assess the methodology for evaluating retrieval accuracy and quality. This may range from no evaluation (Minimal), to periodic review of retrieval quality (Standard), to regular evaluation of retrieval accuracy and relevance (Enhanced). Document the evaluation methodology, metrics used, improvement processes, and governance oversight of retrieval quality management.",2,

Edge Case Management Framework,4.1.9,"Detail the process established for handling edge cases in model outputs. This may range from no process (Minimal), to periodic review (Standard), to documented review process for edge cases (Enhanced). Document the identification, documentation, resolution procedures, and governance oversight for edge case management.",2,

Knowledge Base Quality Management,4.1.10,"Evaluate the management approach for knowledge base quality and coverage. This may range from no assessment (Minimal), to periodic review (Standard), to regular assessment of knowledge base quality and coverage (Enhanced). Document the assessment methodology, quality metrics, improvement processes, and governance oversight of knowledge base management.",2,

Human Oversight Documentation,4.1.11,"Provide comprehensive documentation of all human oversight mechanisms implemented across the AI solution lifecycle. Detail the specific human touchpoints in development, testing, deployment, operation, and monitoring. Identify key roles, responsibilities, required expertise, and governance structures supporting human oversight.",2,

Override Capability Assessment,4.1.12,"Evaluate the capabilities and governance for human override of AI-generated outputs. Document the criteria for override decisions, authorization requirements, execution procedures, documentation standards, and governance controls. Assess the effectiveness of override mechanisms through historical analysis of override instances.",2,

Output Auditing Framework,4.1.13,"Detail the framework established for auditing model outputs. Document audit scope, methodology, frequency, sampling approach, documentation standards, and governance oversight. Assess the effectiveness of the audit framework in identifying output issues and driving quality improvements.",2,

Model Materiality Matrix Application,4.1.14,"Apply the Model Materiality matrix to determine the appropriate materiality level based on the combination of HITL level (Minimal/Standard/Enhanced) and Model Use (High/Medium/Low). The matrix combines these factors to produce a final Materiality Rating (High/Medium/Low/Very Low). Document the specific matrix intersections supporting the assigned rating.",3,

Materiality Justification Analysis,4.1.15,"Provide a comprehensive justification for the assigned Materiality Rating, referencing specific evidence from the HITL and Model Use assessments. This rating directly impacts the Model Tier assignment and governance requirements. Document how the materiality rating aligns with the AI solution's business criticality and risk profile.",3,

5. Model Tier Assessment,,,,"Model Tier is determined by combining Model Complexity and Model Materiality according to the specified tiering matrix. The Model Tier determines the level of governance and controls required."

Tier Matrix Application,5.1.1,"Apply the Model Tier matrix to determine the appropriate Model Tier based on the Model Complexity (Section 3) and Model Materiality (Section 4) assessments. The matrix combines Complexity (High/Medium/Low) and Materiality (High/Medium/Low/Very Low) to assign a Tier from 1 to 4, with Tier 1 representing the highest level of governance requirements and Tier 4 the lowest. Document the specific matrix intersection supporting the assigned tier.",3,

Tier Justification Documentation,5.1.2,"Provide a comprehensive justification for the proposed Model Tier assignment, referencing specific factors from the Complexity and Materiality assessments that influenced the determination. Document how the assigned Tier reflects the overall risk profile of the model and aligns with organizational governance standards. Include any supplemental considerations that influenced the tier determination.",3,

Governance Requirement Mapping,5.1.3,"Detail the specific governance implications of the assigned Model Tier. Document the required: documentation standards, review frequency, testing protocols, validation requirements, approval authorities, monitoring expectations, and reporting obligations. Ensure alignment with organizational governance standards for the assigned Tier and develop an implementation plan for meeting these requirements.",2,

Alternative Tier Consideration Documentation,5.1.4,"If alternative Tier classifications were considered, document the comparative analysis and rationale for the final determination. Detail the risk factors evaluated under different Tier scenarios and provide the business justification for the selected classification. Include input from relevant stakeholders in the justification process.",2,

Business Impact Analysis,5.1.5,"Assess the business impact of the assigned Model Tier by documenting: resource requirements, timeline implications, operational constraints, and compliance obligations. Develop an implementation plan addressing these requirements and identifying responsible parties, timelines, and success metrics for Tier compliance.",2,

Tier Compliance Gap Analysis,5.1.6,"Conduct a gap analysis between current governance practices and the requirements of the assigned Model Tier. Document all identified gaps, mitigation strategies, implementation timelines, and responsible parties. Develop a monitoring framework to track progress toward full Tier compliance.",3,

Tier Reassessment Framework,5.1.7,"Establish a framework for reassessing the Model Tier assignment over time. Document the triggers that would prompt a reassessment, the required approval process for tier changes, and the governance oversight of the Tier classification. Include the communication plan for notifying stakeholders of tier reassessments or changes.",2,

6. Confirmation,,,,"A formal confirmation from the Model Identification Owner(s) or Owner of the Model must be provided to establish agreement with the AI Technology assessment including Model Tier assignment."

Owner Confirmation Documentation,6.1.1,"Provide formal confirmation that the Model Identification Owner(s) or Owner of the Model has reviewed and agreed with the comprehensive AI Technology assessment, including the Model Tier assignment. This confirmation, submitted via attached email or equivalent documentation, establishes accountability for the assessment. Document the date of confirmation and the specific aspects reviewed and approved.",3,

Assessment Completeness Verification,6.1.2,"Verify and document that all sections of the assessment template have been completed with appropriate detail and supporting evidence. For any sections deemed not applicable, provide supporting rationale explaining why the section does not apply. Document the verification process, responsible parties, and completeness metrics.",2,

Review and Challenge Process Documentation,6.1.3,"Detail the review and challenge process applied to this assessment. Document reviewer identities, areas of focus, challenge outcomes, and resolution of any discrepancies. This documentation ensures the assessment has been subjected to appropriate scrutiny and validates the thoroughness of the evaluation process.",2,

Final Determination Record,6.1.4,"Document the final determination regarding Model classification, Complexity, Materiality, and Tier assignment. This serves as the authoritative record for governance purposes and establishes the baseline for ongoing model governance. Include effective dates and review timelines for each determination.",3,

Approval Documentation Completeness,6.1.5,"Confirm and document that all required approvals have been obtained according to governance requirements. Document approver identities, approval dates, and any conditions attached to the approvals. This documentation establishes the official status of the assessment and authorizes implementation of the associated governance framework.",2,

Implementation Plan Development,6.1.6,"Develop a comprehensive implementation plan for meeting the governance requirements associated with the Model determination and Tier assignment. Document action items, responsible parties, timelines, success metrics, and monitoring mechanisms. Include communication protocols for keeping stakeholders informed of implementation progress.",2,

Ongoing Governance Framework,6.1.7,"Establish the ongoing governance framework for the Model based on its classification and Tier assignment. Document governance meeting frequency, standing agenda items, participant roles, reporting requirements, and issue escalation procedures. Include communication protocols for governance activities and decision documentation standards.",3,

7. Additional Risk Assessments,,,, 

Bias Risk Comprehensive Assessment,7.1.1,"Conduct and document a comprehensive assessment of potential bias risks in the AI solution. Evaluate whether the AI solution requires training with information that might cause bias or discrimination (gender, race, ethnicity, age, disability, geographic information, etc.). Detail potential bias sources, risk levels, business implications, and mitigation strategies implemented to address identified risks.",3,

Bias Control Framework Evaluation,7.1.2,"Document all bias and fairness risk controls/metrics implemented in the model. Detail how these controls have been validated through independent testing. Include control mechanisms, testing methodology, effectiveness measures, and ongoing monitoring processes. Assess the adequacy of controls relative to identified bias risks and document any enhancement recommendations.",2,

External Bias Audit Documentation,7.1.3,"If applicable, document any external independent bias audits conducted for this AI solution. Include audit scope, methodology, key findings, remediation actions, and validation of remediation effectiveness. If no external audit has been performed, assess the need for such an audit based on the model's risk profile and usage context.",2,

Bias Monitoring Framework,7.1.4,"Detail the process established for ongoing evaluation of bias & fairness risk. Document the evaluation methodology, metrics tracked, assessment frequency, reporting mechanisms, and governance oversight. Include response protocols for addressing newly identified bias issues and the process for incorporating evolving industry standards for bias detection and mitigation.",2,

Transparency and Explainability Assessment,7.2.1,"Document how the AI solution is evaluated for transparency and explainability. Detail whether this evaluation is conducted internally, by an independent party, or by the service provider. Include evaluation methodology, standards applied, key findings, and enhancement plans. Assess the adequacy of transparency relative to the model's use case and risk profile.",2,

Explainability Framework Documentation,7.2.2,"Detail the explainability metrics and processes established to explain how the AI solution works when requested. Document the technical approach to explainability, user-facing explanation methodologies, and validation procedures that ensure explanations are accurate and comprehensible to the intended audience. Include governance controls specific to explainability management.",2,

Documentation Comprehensiveness Assessment,7.2.3,"Evaluate the completeness of internal documentation owned and managed by the AI solution owner. Verify and document that the documentation includes: ownership information, development process, data sources, intended use, incident management procedures, and other required elements. Assess documentation accessibility, update processes, and governance controls.",2,

Accuracy Evaluation Methodology,7.3.1,"Document the methodology used to evaluate the AI solution for output accuracy against intended use. Detail whether this evaluation is conducted internally, by an independent party, or by the service provider. Include evaluation scope, methodology, performance metrics, acceptance criteria, and governance oversight of the accuracy assessment process.",2,

Error Management Framework Documentation,7.3.2,"Detail all validations built to identify and correct errors in outputs and detect unexpected model changes. Document monitoring systems, correction workflows, effectiveness measures, and governance oversight. Include historical analysis of error detection performance and effectiveness of remediation procedures.",2,

Hallucination Risk Management Assessment,7.3.3,"Document the approach for evaluating and mitigating hallucination risk in the AI solution. Detail detection methods, controls, mitigation strategies, and effectiveness measures. Include governance oversight of hallucination management and escalation procedures for significant hallucination incidents. Assess the adequacy of controls relative to the model's usage context and potential impact of hallucinations.",3,

Stress Testing Documentation,7.4.1,"Document all stress test scenarios conducted to verify AI solution performance under sudden changes in business conditions or inputs. Include test design, execution methodology, results analysis, and remediation actions for identified vulnerabilities. Assess the comprehensiveness of stress testing relative to the model's risk profile and usage context.",3,

Stability Analysis Framework,7.4.2,"Detail the scenario, sensitivity, or benchmarking analysis performed to assess stability and robustness. Document testing methodology, scenarios evaluated, performance metrics, acceptance criteria, and remediation actions for identified issues. Include the governance framework for stability assessment and the process for incorporating findings into model enhancements.",2,

Backtesting Methodology Documentation,7.4.3,"Document the AI solution backtesting methodology, including performance metrics, execution frequency, result analysis, and incorporation of findings into model improvements. Assess the adequacy of backtesting relative to the model's complexity and materiality. Include governance oversight of the backtesting process and documentation standards for backtesting results.",2,

Cyber Security Control Framework,7.5.1,"Document all security measures implemented to protect against adversarial attacks or prompt injection vulnerabilities. Detail control mechanisms, testing procedures, effectiveness measures, and alignment with organizational security standards. Include security incident history, remediation actions, and enhancements implemented based on security events or testing.",3,

Cloud Security Assessment Documentation,7.5.2,"If cloud-deployed, document the comprehensive cloud security assessment conducted for this AI solution. Include assessment scope, methodology, key findings, remediation actions, and validation of control effectiveness. Detail ongoing cloud security monitoring, compliance with cloud security standards, and governance oversight of cloud security management.",2,

Access Control Framework Documentation,7.5.3,"Detail all controls implemented to prevent unauthorized access or modification of the model or its parameters. Document authentication mechanisms, authorization frameworks, privilege management, access reviews, and audit procedures. Include security incident history related to access controls and remediation actions implemented in response to identified vulnerabilities.",2,

Incident Response Plan Documentation,7.5.4,"Document the incident response plan established for AI-related security incidents. Detail detection mechanisms, classification criteria, escalation procedures, response protocols, and recovery plans. Include tabletop exercise results, lessons learned from actual or simulated incidents, and plan update processes based on evolving threats or organizational changes.",2,

8. Regulatory and Compliance Assessment,,,,

Regulatory Compliance Documentation,8.1.1,"Document the comprehensive assessment of applicable laws and regulations relevant to this AI solution. Detail compliance evaluation methodology, identified requirements, implementation measures, and validation procedures. Include the governance framework for ongoing compliance management and the process for monitoring and adapting to regulatory changes.",3,

Ethics Framework Assessment,8.1.2,"Detail the ethical framework applied to the AI solution development and operation. Document ethical principles, implementation mechanisms, assessment methodology, and governance oversight. Include ethical risk assessment, mitigation strategies, and the process for addressing ethical dilemmas that may arise during model operation.",2,

Privacy Impact Assessment,8.1.3,"Document the privacy impact assessment conducted for this AI solution. Detail data privacy requirements, implementation measures, validation procedures, and ongoing monitoring. Include data protection mechanisms, consent management, data minimization approaches, and alignment with organizational privacy standards and applicable regulations.",3,

Model Risk Management Compliance,8.1.4,"Evaluate compliance with organizational Model Risk Management standards and procedures. Document alignment with governance requirements, identified gaps, remediation plans, and validation of compliance. Include the governance framework for ongoing model risk management and the process for adapting to changes in organizational standards.",2,

Third-Party Risk Assessment,8.1.5,"If the AI solution incorporates third-party components or services, document the comprehensive risk assessment of these dependencies. Detail vendor assessment methodology, identified risks, mitigation measures, contractual protections, and ongoing monitoring. Include contingency plans for third-party service disruptions or quality issues.",3,

Documentation Standards Compliance,8.1.6,"Assess compliance with organizational documentation standards for AI solutions. Document alignment with requirements, identified gaps, remediation plans, and validation of compliance. Include the governance framework for documentation management and the process for adapting to changes in documentation standards.",2,

9. Operational Readiness Assessment,,,,

Implementation Planning Documentation,9.1.1,"Detail the comprehensive implementation plan for the AI solution. Document key milestones, resource requirements, dependencies, risk management, and governance oversight. Include the change management approach, stakeholder communication plan, and success criteria for implementation.",2,

Training Program Assessment,9.1.2,"Document the training program developed for system operators, business users, and governance stakeholders. Detail training objectives, curriculum design, delivery methodology, competency assessment, and refresher training requirements. Include evaluation of training effectiveness and the process for updating training based on operational feedback.",2,

Operational Support Framework,9.1.3,"Detail the operational support framework established for the AI solution. Document support levels, response times, escalation procedures, knowledge management, and continuous improvement processes. Include service level agreements, performance metrics, and governance oversight of operational support.",2,

Business Continuity Planning,9.1.4,"Document the business continuity and disaster recovery plans developed for the AI solution. Detail risk assessment, recovery strategies, testing methodology, documentation standards, and governance oversight. Include test results, lessons learned, and plan update processes based on evolving business requirements or technological changes.",3,

Performance Monitoring Framework,9.1.5,"Detail the comprehensive performance monitoring framework established for the AI solution. Document monitored metrics, thresholds, alerting mechanisms, response procedures, and governance oversight. Include historical performance analysis, trend identification, and the process for incorporating monitoring insights into model enhancements.",2,

Change Management Process Documentation,9.1.6,"Document the change management process established for the AI solution. Detail change categories, impact assessment methodology, approval requirements, testing standards, implementation procedures, and rollback capabilities. Include the governance framework for change management and documentation standards for change history.",2,

10. Lifecycle Management Assessment,,,,

Development Lifecycle Documentation,10.1.1,"Document the comprehensive development lifecycle for the AI solution. Detail requirements gathering, design methodology, development standards, testing approach, deployment procedures, and performance validation. Include governance gates, documentation requirements, and quality assurance measures at each lifecycle stage.",2,

Maintenance Strategy Assessment,10.1.2,"Detail the maintenance strategy established for the AI solution. Document scheduled maintenance activities, performance optimization approaches, technical debt management, and governance oversight. Include maintenance history, effectiveness metrics, and the process for evolving the maintenance strategy based on operational experience.",2,

Retirement Planning Documentation,10.1.3,"Document the retirement planning considerations for the AI solution. Detail trigger criteria, impact assessment methodology, data archiving requirements, stakeholder communication plan, and governance oversight. Include the transition strategy for successor systems and compliance considerations for data retention or destruction.",2,

Version Control Framework,10.1.4,"Detail the version control framework established for the AI solution. Document versioning methodology, change tracking, deployment validation, rollback capabilities, and governance oversight. Include historical version analysis, trend identification, and the process for managing concurrent development streams.",2,

Audit History Management,10.1.5,"Document the audit history management approach for the AI solution. Detail audit scope, frequency, methodology, documentation standards, and remediation tracking. Include audit history analysis, trend identification, and the process for incorporating audit findings into model governance and enhancement.",3,
