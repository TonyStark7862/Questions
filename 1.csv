Risk category and sub-category,Ref.,Question,Risk Scores,Justification

1. Description of AI Technology,,,"1=Low Risk, 2=Medium Risk, 3=High Risk",

Purpose of AI Technology,1.0.1,Describe the purpose of the AI Technology/Model. What business problem does it solve?,1,
Purpose of AI Technology,1.0.2,How does this AI solution align with business requirements and strategic objectives?,1,
Purpose of AI Technology,1.0.3,What are the expected business outcomes and value proposition of this AI solution?,1,
Development Team,1.0.4,Specify the desk/team that developed the AI technology. Include team composition and roles.,1,
Development Team,1.0.5,What AI/ML expertise does the development team possess? Detail relevant experience and qualifications.,1,
Development Team,1.0.6,Was the development outsourced or developed in partnership with external vendors? If yes, provide details.,2,
Target Users,1.0.7,Specify who are the users and at which regions the AI Technology will be deployed.,1,
Target Users,1.0.8,Are there region-specific adaptations or considerations in the model deployment?,1,
Target Users,1.0.9,How will user access and permissions be managed across different user groups and regions?,1,

1.1 Objectives,,,,

Business Objectives,1.1.1,What are the primary business objectives of the AI solution? How are these objectives measured?,1,
Business Objectives,1.1.2,Does the application have a well-articulated application scope? Provide details.,1,
Business Objectives,1.1.3,How do these objectives align with organizational goals and strategy?,1,
Technical Objectives,1.1.4,What are the technical objectives of the model? Include performance targets and metrics.,2,
Technical Objectives,1.1.5,How were these technical objectives determined? Are they aligned with business requirements?,2,
Success Criteria,1.1.6,What defines success for this AI Technology? How is success measured?,1,
Success Criteria,1.1.7,Are there specific KPIs established to measure the model's performance against objectives?,1,
Use Case Definition,1.1.8,What specific use cases does this AI Technology address? Provide detailed examples.,1,
Use Case Definition,1.1.9,Are there any use cases explicitly excluded from the scope? Why were they excluded?,2,

1.2 Assumptions and Limitations,,,,

Key Assumptions,1.2.1,What key assumptions were made during model development? Complete the assumptions table with implications and justifications.,2,
Key Assumptions,1.2.2,How were these assumptions validated? Provide evidence supporting their validity.,2,
Key Assumptions,1.2.3,Are there regulatory interpretations or domain-specific assumptions incorporated into the model?,3,
Data Source Assumptions,1.2.4,Does the AI Model use a single source of news/data? What are the implications for potentially missing risk events?,2,
Data Source Assumptions,1.2.5,Why was this data source selected over alternatives? Justify the data source selection.,2,
Data Source Assumptions,1.2.6,What is the reliability and completeness of the selected data sources? How is this assessed?,2,
Model Limitations,1.2.7,What are the known limitations of the model regarding specific regions, languages, or business domains?,2,
Model Limitations,1.2.8,How do these limitations impact the reliability of the model's outputs?,2,
Model Limitations,1.2.9,What communication process ensures users understand these limitations?,2,
Output Bias,1.2.10,Are there potential biases in model outputs (e.g., towards larger organizations or US-based entities)? What is their severity (High/Medium/Low)?,3,
Output Bias,1.2.11,What analysis was conducted to identify these biases? Provide methodology and results.,3,
Output Bias,1.2.12,How might these biases impact decision-making based on model outputs?,3,
Mitigation Controls,1.2.13,What mitigating factors or guardrails have been implemented to address identified limitations?,2,
Mitigation Controls,1.2.14,How effective are these mitigating controls? How is their effectiveness measured?,2,
Mitigation Controls,1.2.15,Is there a process for identifying new limitations and implementing additional controls?,2,

1.3 Inputs,,,,

Input Inventory,1.3.1,Detail the full list of the AI Technology's inputs. Include all data sources and input parameters.,2,
Input Inventory,1.3.2,What pre-processing requirements (e.g., tokenization, named entity recognition) are applied to inputs?,2,
Input Inventory,1.3.3,How are inputs validated before processing? Detail validation procedures and quality checks.,2,
Opaque Parameters,1.3.4,Identify all opaque parameters or expert judgments used in the model. What controls exist around these parameters?,2,
Opaque Parameters,1.3.5,How are changes to opaque parameters tracked and documented?,2,
Opaque Parameters,1.3.6,What is the governance process for approving changes to expert judgment inputs?,3,
Parameter Updates,1.3.7,What is the update frequency for model parameters and expert judgments? Is this sufficient for the intended use case?,2,
Parameter Updates,1.3.8,How are parameter updates validated before implementation?,2,
Parameter Updates,1.3.9,Is there a rollback procedure if parameter updates cause unexpected behavior?,2,
RAG Implementation,1.3.10,Is a Retrieval-Augmented Generation (RAG) process used for optimizing the output of the model?,2,
RAG Implementation,1.3.11,If RAG is used, describe the retrieval methodology and document sources.,2,
RAG Implementation,1.3.12,How is the quality and relevance of retrieved information evaluated and maintained?,2,
Internal Data Sources,1.3.13,Are any XYZ internal data sources (Sharepoint, Jira, Gitlab, Confluence) used? Detail security controls.,2,
Internal Data Sources,1.3.14,How is access to internal data sources managed and monitored?,2,
Internal Data Sources,1.3.15,What protocols ensure confidential information is appropriately handled?,3,
Feeder Models,1.3.16,List all feeder AI Models or Agents (internal or third party) whose outputs are used by this AI Model.,3,
Feeder Models,1.3.17,How is the quality of feeder model outputs validated?,3,
Feeder Models,1.3.18,What contingency exists if a feeder model becomes unavailable or produces unreliable results?,3,

1.4 Calibration,,,,

Calibration Approach,1.4.1,Detail the calibration methodology for AI Technology parameters and/or methodology with implementation details and rationale.,2,
Calibration Approach,1.4.2,How was the calibration approach determined? Why is it appropriate for this model?,2,
Calibration Approach,1.4.3,What metrics are used to evaluate calibration effectiveness?,2,
Calibration Frequency,1.4.4,What is the frequency of model calibration? Provide justification for this cadence.,2,
Calibration Frequency,1.4.5,Are there trigger events that would initiate off-cycle calibration activities?,2,
Calibration Frequency,1.4.6,Who is responsible for determining when calibration should occur?,1,
External Knowledge Integration,1.4.7,How is external knowledge integrated into the model (e.g., through RAG implementation, fine-tuning, or other methods)?,2,
External Knowledge Integration,1.4.8,What sources of external knowledge are used? How are these sources selected and validated?,2,
External Knowledge Integration,1.4.9,How frequently is external knowledge updated in the model?,2,
Fine-tuning Process,1.4.10,If the model is fine-tuned, describe the fine-tuning dataset, process, and validation methodology.,2,
Fine-tuning Process,1.4.11,What measures prevent overfitting during fine-tuning?,2,
Fine-tuning Process,1.4.12,How are fine-tuning results evaluated and approved before deployment?,3,
Validation Procedures,1.4.13,How is calibration effectiveness measured and validated? What thresholds determine successful calibration?,2,
Validation Procedures,1.4.14,Who reviews and approves calibration results?,2,
Validation Procedures,1.4.15,How are calibration failures addressed?,2,

1.5 Numerical Method,,,,

Method Description,1.5.1,Detail the numerical methods used, including analytical and semi-analytical approaches.,2,
Method Description,1.5.2,Why were these numerical methods selected over alternatives? Justify the selection.,2,
Method Description,1.5.3,Have these methods been validated for similar applications? Provide evidence.,2,
Non-analytical Methods,1.5.4,Does the approach use non-analytical methods (e.g., Monte Carlo Simulations, Finite Difference Method)? Provide details.,3,
Non-analytical Methods,1.5.5,How are non-analytical methods implemented and optimized?,3,
Non-analytical Methods,1.5.6,What validation ensures non-analytical methods produce reliable results?,3,
Numerical Inputs,1.5.7,What are the recommended levels/values for numerical inputs? How were these determined?,2,
Numerical Inputs,1.5.8,How sensitive is the model to variations in numerical inputs? Provide sensitivity analysis.,2,
Numerical Inputs,1.5.9,Are there guardrails to prevent invalid numerical inputs?,2,
Accuracy Requirements,1.5.10,How is numerical accuracy validated and maintained? Detail testing methodologies.,2,
Accuracy Requirements,1.5.11,What are the acceptable error margins for numerical outputs?,2,
Accuracy Requirements,1.5.12,How are numerical accuracy issues identified and resolved?,2,
Implementation Details,1.5.13,Describe the technical implementation of numerical methods. Include optimization techniques used.,2,
Implementation Details,1.5.14,How is computational efficiency balanced with numerical accuracy?,2,
Implementation Details,1.5.15,What hardware or infrastructure requirements support the numerical methods used?,1,

1.6 Outputs,,,,

Output Description,1.6.1,Provide a comprehensive description of the outputs of the AI Technology and how these are used by users.,2,
Output Description,1.6.2,How are outputs formatted and presented to users? Include all output types and formats.,1,
Output Description,1.6.3,What metadata accompanies the outputs to aid interpretation?,1,
UI Content Generation,1.6.4,Does the model generate content for UI users? Describe the nature and format of this content.,1,
UI Content Generation,1.6.5,How is UI content quality controlled before presentation to users?,2,
UI Content Generation,1.6.6,Are there content moderation mechanisms to prevent inappropriate outputs?,3,
Query Generation,1.6.7,Does the model generate SQL queries from natural language prompts? How is query accuracy and security ensured?,2,
Query Generation,1.6.8,What safeguards prevent generation of harmful or resource-intensive queries?,3,
Query Generation,1.6.9,How are generated queries validated before execution?,3,
Output Interpretation,1.6.10,What guidance is provided to users on interpreting model outputs? Are confidence levels included?,2,
Output Interpretation,1.6.11,How are output limitations and assumptions communicated to users?,2,
Output Interpretation,1.6.12,Is there a process for users to request clarification on outputs?,1,
Output Usage,1.6.13,How are model outputs integrated into business processes? Detail all integration points and dependencies.,2,
Output Usage,1.6.14,What controls ensure appropriate use of model outputs by downstream systems or users?,2,
Output Usage,1.6.15,How is output usage tracked and monitored?,1,

2. Model Assessment,,,,

Model Definition,2.0.1,"Is this AI Technology a Model per the definition: A quantitative method, system or approach that applies statistical, economic, financial, or mathematical theories to process input data into estimates that are inherently uncertain?",3,
Model Definition,2.0.2,Does the AI solution process input data into quantitative or qualitative estimates?,3,
Model Definition,2.0.3,Are the outputs inherently uncertain (estimates, forecasts, predictions, or projections)?,3,
Specific Assumptions,2.1.1,Are there approach-specific assumptions that qualify this as a Model? Reference Section 1.2.,3,
Specific Assumptions,2.1.2,How do these assumptions influence the model's behavior and outputs?,2,
Specific Assumptions,2.1.3,Are these assumptions documented and reviewed regularly?,2,
Input Uncertainty,2.1.4,Does the approach rely on inputs that are uncertain, including wholly/partially qualitative, opaque, or based on expert judgment?,3,
Input Uncertainty,2.1.5,What percentage of inputs are based on expert judgment or qualitative assessment?,2,
Input Uncertainty,2.1.6,How is input uncertainty measured and accounted for in the model?,2,
Dependency on Other Models,2.1.7,Does the model use outputs from other models as inputs? List all model dependencies.,3,
Dependency on Other Models,2.1.8,How does uncertainty propagate through these model dependencies?,3,
Dependency on Other Models,2.1.9,What controls manage risks from model dependencies?,2,
Calibration Needs,2.1.10,Do any of the approach's inputs and/or methodology require calibration as referenced in Section 1.4?,2,
Calibration Needs,2.1.11,How frequently does calibration need to occur to maintain model accuracy?,2,
Calibration Needs,2.1.12,What triggers recalibration of the model or its components?,2,
Numerical Methods,2.1.13,Is the approach based on a non-analytical method (Monte Carlo Simulations, Finite Difference Method, etc.)?,3,
Numerical Methods,2.1.14,What level of complexity do these non-analytical methods introduce?,2,
Numerical Methods,2.1.15,How are the results of non-analytical methods validated?,2,
Model Classification,2.1.16,Based on the assessment, should this AI Technology be classified as a Model or non-Model?,3,
Model Classification,2.1.17,If at least one condition is applicable but the approach is not identified as a Model, provide justification.,3,
Model Classification,2.1.18,Conclusion by the owner of the calculation: Model or non-Model? Provide supporting rationale.,3,

3. Model Complexity Assessment,,,,

Industry Acceptance,3.1.1,Is this a non-standard Model based on a special set of Model assumptions and/or a large set of non-standard inputs? Detail all non-standard elements.,3,
Industry Acceptance,3.1.2,How does this model differ from industry-standard approaches for similar purposes?,2,
Industry Acceptance,3.1.3,Has any benchmarking against industry practices been conducted? Provide results.,2,
Input Sensitivity,3.1.4,How sensitive is the Model to Model assumptions and Expert Judgment inputs? Provide detailed sensitivity analysis.,3,
Input Sensitivity,3.1.5,Which inputs drive the most significant changes in model outputs?,3,
Input Sensitivity,3.1.6,What procedures control for high sensitivity to specific inputs?,2,
Assumption Suitability,3.1.7,Are Model assumptions and inputs fully suitable to Model intended use? Identify any inadequacies.,2,
Assumption Suitability,3.1.8,How was the suitability of assumptions validated? Provide evidence.,2,
Assumption Suitability,3.1.9,Is there a process to review assumption suitability as business conditions change?,2,
Hyperparameter Complexity,3.1.10,Does the algorithm contain a high number of hyperparameters? List all key hyperparameters and their optimization process.,3,
Hyperparameter Complexity,3.1.11,What hyperparameter tuning methodology is used? Describe the optimization approach.,2,
Hyperparameter Complexity,3.1.12,How are optimal hyperparameter values determined and validated?,2,
Data Complexity,3.1.13,What is the nature and quality of data inputs? Assess complexity of data structures and quality of inputs.,3,
Data Complexity,3.1.14,How many unstructured or complex data sources are used? Detail all such sources.,3,
Data Complexity,3.1.15,What processes ensure data quality and appropriateness for model inputs?,2,
Feature Complexity,3.1.16,How many features/variables does the Model use? Does a large number of features increase Model Risk?,2,
Feature Complexity,3.1.17,What feature selection methodology was employed? Justify the approach.,2,
Feature Complexity,3.1.18,How are feature interactions and dependencies managed within the model?,3,
Performance Testing,3.1.19,How does the Model perform under current market or extreme conditions? Provide stress testing results.,3,
Performance Testing,3.1.20,What extreme scenarios were tested? How were these scenarios determined?,3,
Performance Testing,3.1.21,What performance degradation is acceptable under extreme conditions?,2,
Infrastructure Quality,3.1.22,Is there an automated Model infrastructure with straight-through processing? Detail automation level.,2,
Infrastructure Quality,3.1.23,How is the model infrastructure documented and maintained?,2,
Infrastructure Quality,3.1.24,What controls prevent operational errors in the model infrastructure?,2,
Model Dependencies,3.1.25,How many feeder Models are used? Detail all dependencies and their complexity.,3,
Model Dependencies,3.1.26,What contingency plans exist for dependency failures?,3,
Model Dependencies,3.1.27,How are dependencies validated and monitored?,2,
Complexity Score,3.1.28,Calculate the total complexity score based on the risk factors above (out of 19). Per scoring: Low (<=5), Medium (6-11), High (>=12).,3,
Complexity Score,3.1.29,Show detailed calculation for each risk factor score.,3,
Complexity Score,3.1.30,What is the proposed classification (Low/Medium/High) based on the total score?,3,

4. Model Materiality Assessment,,,,

4.1 Model Materiality,,,,

Model Use Purpose,4.1.1,What is the purpose of the Model regarding external or internal use? Select appropriate Model Use Rating (High/Medium/Low).,3,
Model Use Purpose,4.1.2,Are model outputs used for external-facing activity (e.g., client, regulator)? Detail all external uses.,3,
Model Use Purpose,4.1.3,Are outputs used for internal facing activity with direct impact on external output (e.g., risk management)?,2,
Model Use Purpose,4.1.4,Are outputs used for internal facing activity aimed at increasing operational efficiencies (e.g., data analytics, chatbot)?,1,
Business Impact,4.1.5,What business functions depend on this model's outputs? Detail criticality of these dependencies.,3,
Business Impact,4.1.6,What would be the business impact of model failure or significant error?,3,
Business Impact,4.1.7,Are there alternative processes if model outputs are unavailable?,2,
Human in the Loop Implementation,4.1.8,What level of human oversight is implemented (Enhanced/Standard/Minimal)? Refer to HITL assessment matrix.,3,
Human in the Loop Implementation,4.1.9,Who performs human oversight activities? Detail their qualifications and training.,2,
Human in the Loop Implementation,4.1.10,How is human oversight documented and validated?,2,
Materiality Determination,4.1.11,Based on Model Use and Human in the Loop assessment, determine the Model Materiality rating (High/Medium/Low).,3,
Materiality Determination,4.1.12,Using the materiality matrix, what is the final materiality rating based on HITL level and Model Use Rating?,3,
Materiality Determination,4.1.13,Provide detailed justification for the materiality rating.,2,

4.1.1 Human in the Loop,,,,

Prompt Engineering,4.1.1.1,What level of prompt engineering is implemented? (Basic/Structured/Advanced),2,
Prompt Engineering,4.1.1.2,Is there a prompt management system with version control?,2,
Prompt Engineering,4.1.1.3,How are prompts tested and validated before implementation?,2,
Output Review,4.1.1.4,Is there systematic review of outputs? Detail review frequency and methodology.,2,
Output Review,4.1.1.5,What sampling methodology is used for output review?,2,
Output Review,4.1.1.6,How are review findings documented and addressed?,2,
Feedback Process,4.1.1.7,Is there a feedback collection process from end users? Describe the implementation.,2,
Feedback Process,4.1.1.8,How is user feedback incorporated into model improvements?,2,
Feedback Process,4.1.1.9,Is there a formal feedback review and prioritization process?,2,
Monitoring Framework,4.1.1.10,How comprehensive is the output monitoring and quality control? Detail procedures.,2,
Monitoring Framework,4.1.1.11,What metrics are tracked in the monitoring framework?,2,
Monitoring Framework,4.1.1.12,Who reviews monitoring results and how frequently?,2,
Prompt Optimization,4.1.1.13,Is there active prompt optimization based on user feedback? Describe methodology.,2,
Prompt Optimization,4.1.1.14,How frequently are prompts optimized?,2,
Prompt Optimization,4.1.1.15,How is prompt optimization effectiveness measured?,2,
Edge Case Management,4.1.1.16,Is there a documented review process for edge cases? Detail procedures.,2,
Edge Case Management,4.1.1.17,How are edge cases identified and cataloged?,2,
Edge Case Management,4.1.1.18,What process ensures edge cases inform model improvements?,2,
SME Involvement,4.1.1.19,Is there systematic collection and incorporation of SME feedback? Detail process.,2,
SME Involvement,4.1.1.20,How are SMEs selected and engaged in the feedback process?,2,
SME Involvement,4.1.1.21,How is SME feedback prioritized and implemented?,2,
Knowledge Management,4.1.1.22,How regularly is the knowledge base quality and coverage assessed? Detail process.,2,
Knowledge Management,4.1.1.23,What metrics evaluate knowledge base quality and coverage?,2,
Knowledge Management,4.1.1.24,How are knowledge gaps identified and addressed?,2,

4.1.2 Model Use,,,,

External Usage,4.1.2.1,Are model outputs used for external facing activity (e.g., client, regulator)? Detail all scenarios.,3,
External Usage,4.1.2.2,What controls ensure appropriate external use of model outputs?,3,
External Usage,4.1.2.3,How are external stakeholders informed about model limitations?,3,
Internal Impact,4.1.2.4,Are outputs used for internal facing activity with direct impact on external output (e.g., risk management)?,2,
Internal Impact,4.1.2.5,What is the pathway from model output to external impact? Detail all touchpoints.,2,
Internal Impact,4.1.2.6,What controls exist at each step of this pathway?,2,
Operational Efficiency,4.1.2.7,Are outputs used for internal facing activity aimed at increasing operational efficiencies (e.g., data analytics)?,1,
Operational Efficiency,4.1.2.8,How is the operational efficiency impact measured?,1,
Operational Efficiency,4.1.2.9,What business functions benefit from these efficiency improvements?,1,

5. Model Tier (Models Only),,,,

5.1 Model Tier,,,,

Tier Determination,5.1.1,Based on Model Complexity score (Section 3) and Model Materiality (Section 4), determine the proposed Model Tier using the tiering matrix.,3,
Tier Determination,5.1.2,Show how the complexity score and materiality rating map to the tiering matrix.,3,
Tier Determination,5.1.3,What is the proposed Model Tier (1-4)? Lower numbers indicate higher governance requirements.,3,
Governance Requirements,5.1.4,Based on the assigned Model Tier, what specific governance requirements apply to this model?,2,
Governance Requirements,5.1.5,Are there any tier-specific documentation or testing requirements?,2,
Governance Requirements,5.1.6,What is the required review and approval frequency based on the assigned tier?,2,
Justification,5.1.7,Provide detailed justification for the proposed Model Tier assignment.,3,
Justification,5.1.8,Are there any special considerations that influenced the tier assignment?,2,
Justification,5.1.9,Has this tier assignment been reviewed by appropriate governance stakeholders?,2,

6. Confirmation,,,,

Owner Confirmation,6.0.1,Has the Model Identification Owner(s) or Owner of the Model confirmed agreement with the AI Technology assessment?,3,
Owner Confirmation,6.0.2,Provide the confirmation date and method (e.g., email, signed document).,2,
Owner Confirmation,6.0.3,Are there any conditions or qualifications to the owner's confirmation?,2,
Documentation Completeness,6.0.4,Is all required documentation complete according to governance standards?,3,
Documentation Completeness,6.0.5,Where is the documentation stored and how is it versioned?,2,
Documentation Completeness,6.0.6,What process ensures documentation remains current as the model evolves?,2,
Tier Assignment Approval,6.0.7,Has the Model Tier assignment been formally approved? By whom?,3,
Tier Assignment Approval,6.0.8,What evidence supports the tier assignment approval?,2,
Tier Assignment Approval,6.0.9,Is there a process for reviewing and potentially changing the tier assignment over time?,2,

7. Additional Risk Assessments,,,,

Bias Assessment,7.0.1,Will the AI solution require training with information that might cause bias or discrimination? Detail potential sources.,3,
Bias Assessment,7.0.2,What bias and fairness risk controls/metrics are in place? Have these been independently validated?,3,
Bias Assessment,7.0.3,Is there a process for evaluating bias risk on at least an annual basis?,2,
Transparency Evaluation,7.0.4,How is the AI solution evaluated for transparency and explainability? Detail methodology.,2,
Transparency Evaluation,7.0.5,Does the AI solution have identified explainability metrics and process for explaining how it works?,2,
Transparency Evaluation,7.0.6,Is there comprehensive documentation of the solution including ownership, development process, etc.?,2,
Accuracy Assessment,7.0.7,How is the AI solution evaluated for accuracy against intended use? Describe evaluation methods.,2,
Accuracy Assessment,7.0.8,What validations exist to identify and correct errors in outputs and detect unexpected changes?,2,
Accuracy Assessment,7.0.9,Is the solution evaluated for hallucination risk? Detail detection and mitigation approaches.,3,
Robustness Testing,7.0.10,Have stress tests been conducted to verify performance under sudden changes in conditions?,3,
Robustness Testing,7.0.11,Has scenario, sensitivity, or benchmarking analysis been performed to prove stability?,2,
Robustness Testing,7.0.12,Is solution backtesting performed? Detail methodology and findings.,2,
Security Controls,7.0.13,What measures protect against adversarial attacks or prompt injection vulnerabilities?,3,
Security Controls,7.0.14,If cloud-deployed, has cloud security assessment been conducted? Provide results.,2,
Security Controls,7.0.15,Is there an incident response plan for AI-related security incidents? Detail procedures.,2,
